kubeVersionOverride: null
nameOverride: null
fullnameOverride: null

image:
  repository: grafana/mimir
  tag: 2.10.4
  pullPolicy: IfNotPresent

global:
  dnsService: kube-dns
  dnsNamespace: kube-system
  clusterDomain: cluster.local
  extraEnv: []
  extraEnvFrom: []
  podAnnotations: {}
  podLabels: {}

serviceAccount:
  create: true

useExternalConfig: false
configStorageType: ConfigMap
externalConfigSecretName: '{{ include "mimir.resourceName" (dict "ctx" . "component" "config") }}'
externalConfigVersion: '0'

vaultAgent:
  enabled: false

mimir:
  config: |
    usage_stats:
      enabled: false
    activity_tracker:
      filepath: /active-query-tracker/activity.log

    alertmanager:
      data_dir: /data
      enable_api: true
      external_url: /alertmanager
      {{- if .Values.alertmanager.zoneAwareReplication.enabled }}
      sharding_ring:
        zone_awareness_enabled: false
      {{- end }}
      {{- if .Values.alertmanager.fallbackConfig }}
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
      {{- end }}

    {{- if .Values.minio.enabled }}
    alertmanager_storage:
      backend: s3
      s3:
        http:
          insecure_skip_verify: true
        access_key_id: {{ .Values.minio.rootUser }}
        bucket_name: {{ include "mimir.minioBucketPrefix" . }}-ruler
        endpoint: ${S3_ENDPOINT}
        insecure: false
        secret_access_key: {{ .Values.minio.rootPassword }}
    {{- end }}

    # This configures how the store-gateway synchronizes blocks stored in the bucket. It uses Minio by default for getting started (configured via flags) but this should be changed for production deployments.
    blocks_storage:
      backend: s3
      bucket_store:
        {{- if index .Values "chunks-cache" "enabled" }}
        chunks_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.chunksCacheAddress" . }}
            max_item_size: {{ mul (index .Values "chunks-cache").maxItemMemory 1024 1024 }}
            timeout: 450ms
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "index-cache" "enabled" }}
        index_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.indexCacheAddress" . }}
            max_item_size: {{ mul (index .Values "index-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        {{- if index .Values "metadata-cache" "enabled" }}
        metadata_cache:
          backend: memcached
          memcached:
            addresses: {{ include "mimir.metadataCacheAddress" . }}
            max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
            max_idle_connections: 150
        {{- end }}
        sync_dir: /data/tsdb-sync
      s3:
        http:
          insecure_skip_verify: true
        access_key_id: AIPISTILRV3AA3YT
        secret_access_key: 5BH2P0RPYRTMJVYT4ZP4S1VTBFDYPIFE
        bucket_name: mimir
        endpoint: minio.s3.svc.cluster.local
        insecure: true
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 1

    compactor:
      compaction_interval: 30m
      deletion_delay: 2h
      max_closing_blocks_concurrency: 2
      max_opening_blocks_concurrency: 4
      symbols_flushers_concurrency: 4
      first_level_compaction_wait_period: 25m
      data_dir: "/data"
      sharding_ring:
        wait_stability_min_duration: 1m
        heartbeat_period: 1m
        heartbeat_timeout: 4m

    frontend:
      parallelize_shardable_queries: true
      {{- if index .Values "results-cache" "enabled" }}
      results_cache:
        backend: memcached
        memcached:
          timeout: 500ms
          addresses: {{ include "mimir.resultsCacheAddress" . }}
          max_item_size: {{ mul (index .Values "results-cache").maxItemMemory 1024 1024 }}
      cache_results: true
      query_sharding_target_series_per_shard: 2500
      {{- end }}
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    frontend_worker:
      grpc_client_config:
        max_send_msg_size: 1073741824 # 1GiB
      {{- if .Values.query_scheduler.enabled }}
      scheduler_address: {{ template "mimir.fullname" . }}-query-scheduler-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- else }}
      frontend_address: {{ template "mimir.fullname" . }}-query-frontend-headless.{{ .Release.Namespace }}.svc:{{ include "mimir.serverGrpcListenPort" . }}
      {{- end }}

    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        replication_factor: 1
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        heartbeat_period: 2m
        heartbeat_timeout: 10m
        {{- if .Values.ingester.zoneAwareReplication.enabled }}
        zone_awareness_enabled: true
        {{- end }}

    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600

    limits:
      # Limit queries to 500 days. You can override this on a per-tenant basis.
      max_total_query_length: 12000h
      # Adjust max query parallelism to 16x sharding, without sharding we can run 15d queries fully in parallel.
      # With sharding we can further shard each day another 16 times. 15 days * 16 shards = 240 subqueries.
      max_query_parallelism: 240
      # Avoid caching results newer than 10m because some samples can be delayed
      # This presents caching incomplete results
      max_cache_freshness: 10m
      ha_replica_label: prometheus_replica
      ha_cluster_label: cluster

    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: true
      join_members:
      - dns+{{ include "mimir.fullname" . }}-gossip-ring.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:{{ include "mimir.memberlistBindPort" . }}
      # default 5s
      packet_write_timeout: 7s

    querier:
      # With query sharding we run more but smaller queries. We must strike a balance
      # which allows us to process more sharded queries in parallel when requested, but not overload
      # queriers during non-sharded queries.
      max_concurrent: 16

    query_scheduler:
      # Increase from default of 100 to account for queries created by query sharding
      max_outstanding_requests_per_tenant: 800

    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.{{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/alertmanager
      enable_api: true
      rule_path: /data

    ruler_storage:
      backend: s3
      s3:
        http:
          insecure_skip_verify: true
        access_key_id: AIPISTILRV3AA3YT
        secret_access_key: 5BH2P0RPYRTMJVYT4ZP4S1VTBFDYPIFE
        endpoint: minio.s3.svc.cluster.local
        bucket_name: mimir-ruler
        insecure: true
      {{- if index .Values "metadata-cache" "enabled" }}
      cache:
        backend: memcached
        memcached:
          addresses: {{ include "mimir.metadataCacheAddress" . }}
          max_item_size: {{ mul (index .Values "metadata-cache").maxItemMemory 1024 1024 }}
      {{- end }}

    runtime_config:
      file: /var/{{ include "mimir.name" . }}/runtime.yaml

    server:
      grpc_server_max_concurrent_streams: 1000
      grpc_server_max_connection_age: 2m
      grpc_server_max_connection_age_grace: 5m
      grpc_server_max_connection_idle: 1m

    store_gateway:
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
        wait_stability_min_duration: 1m
        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
        kvstore:
          prefix: multi-zone/
        {{- end }}
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        {{- if .Values.store_gateway.zoneAwareReplication.enabled }}
        zone_awareness_enabled: true
        {{- end }}

    distributor:
      ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m

  # -- Additional structured values on top of the text based 'mimir.config'. Applied after the text based config is evaluated for templates. Enables adding and modifying YAML elements in the evaulated 'mimir.config'.
  structuredConfig:
    no_auth_tenant: "unknown"

runtimeConfig:
  overrides:
    prom:
      service_overload_status_code_on_rate_limit_enabled: true
      ingestion_rate: 60000
      max_global_series_per_user: 1200000
      max_global_metadata_per_user: 2000
      max_total_query_length: 500d
      out_of_order_time_window: 0
      compactor_blocks_retention_period: 60d
      accept_ha_samples: true
      # ha_replica_label: prometheus_replica
      # ha_cluster_label: prometheus_replica

rbac:
  create: true

alertmanager:
  enabled: false
  # -- Total number of replicas for the alertmanager across all availability zones
  # If alertmanager.zoneAwareReplication.enabled=false, this number is taken as is.
  # Otherwise each zone starts `ceil(replicas / number_of_zones)` number of pods.
  #   E.g. if 'replicas' is set to 4 and there are 3 zones, then 4/3=1.33 and after rounding up it means 2 pods per zone are started.
  replicas: 1

  statefulSet:
    enabled: true

  service:
    annotations: {}
    labels: {}

  # -- Optionally set the scheduler for pods of the alertmanager
  schedulerName: ""

  resources:
    requests:
      cpu: 10m
      memory: 32Mi

  # -- Fallback config for alertmanager.
  # When a tenant doesn't have an Alertmanager configuration, the Grafana Mimir Alertmanager uses the fallback configuration.
  fallbackConfig: |
    receivers:
        - name: default-receiver
    route:
        receiver: default-receiver

  extraArgs: {}

  # Pod Labels
  podLabels: {}

  # Pod Annotations
  podAnnotations: {}

  # -- Pod Disruption Budget for alertmanager, this will be applied across availability zones to prevent losing redundancy
  podDisruptionBudget:
    maxUnavailable: 1

  # -- The name of the PriorityClass for alertmanager pods
  priorityClassName: null

  # -- NodeSelector to pin alertmanager pods to certain set of nodes. This is ignored when alertmanager.zoneAwareReplication.enabled=true.
  nodeSelector: {}
  # -- Pod affinity settings for the alertmanager. This is ignored when alertmanager.zoneAwareReplication.enabled=true.
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints:
    maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway

  annotations: {}
  persistence:
    # SubPath in emptyDir for persistence, only enabled if alertmanager.statefulSet.enabled is false
    subPath:

  persistentVolume:
    # If true and alertmanager.statefulSet.enabled is true,
    # Alertmanager will create/use a Persistent Volume Claim
    # If false, use emptyDir
    enabled: true

    # Alertmanager data Persistent Volume Claim annotations
    #
    annotations: {}

    # Alertmanager data Persistent Volume access modes
    # Must match those of existing PV or dynamic provisioner
    # Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    #
    accessModes:
      - ReadWriteOnce

    # Alertmanager data Persistent Volume size
    #
    size: 1Gi

    # Subdirectory of Alertmanager data Persistent Volume to mount
    # Useful if the volume's root directory is not empty
    #
    subPath: ''

    # Alertmanager data Persistent Volume Storage Class
    # If defined, storageClassName: <storageClass>
    # If set to "-", storageClassName: "", which disables dynamic provisioning
    # If undefined (the default) or set to null, no storageClassName spec is
    #   set, choosing the default provisioner.
    #
    # A per-zone storageClass configuration in `alertmanager.zoneAwareReplication.zones[*].storageClass` takes precedence over this field.
    # storageClass: "-"

  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45

  # -- SecurityContext override for alermeneger pods
  securityContext: {}

  # -- The SecurityContext for alertmanager containers
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]

  # Tolerations for pod assignment
  # ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  # -- updateStrategy of the alertmanager statefulset. This is ignored when alertmanager.zoneAwareReplication.enabled=true.
  statefulStrategy:
    type: RollingUpdate

  terminationGracePeriodSeconds: 60

  initContainers: []
  # Init containers to be added to the alertmanager pod.
  # - name: my-init-container
  #   image: busybox:latest
  #   command: ['sh', '-c', 'echo hello']

  extraContainers: []
  # Additional containers to be added to the alertmanager pod.
  # - name: reverse-proxy
  #   image: angelbarrera92/basic-auth-reverse-proxy:dev
  #   args:
  #     - "serve"
  #     - "--upstream=http://localhost:3100"
  #     - "--auth-config=/etc/reverse-proxy-conf/authn.yaml"
  #   ports:
  #     - name: http
  #       containerPort: 11811
  #       protocol: TCP
  #   volumeMounts:
  #     - name: reverse-proxy-auth-config
  #       mountPath: /etc/reverse-proxy-conf

  extraVolumes: []
  # Additional volumes to the alertmanager pod.
  # - name: reverse-proxy-auth-config
  #   secret:
  #     secretName: reverse-proxy-auth-config

  # Extra volume mounts that will be added to the alertmanager container
  extraVolumeMounts: []

  # Extra env variables to pass to the alertmanager container
  env: []
  extraEnvFrom: []

  # -- Options to configure zone-aware replication for alertmanager
  # Example configuration with full geographical redundancy:
  # rollout_operator:
  #   enabled: true
  # alertmanager:
  #   zoneAwareReplication:
  #     enabled: true
  #     topologyKey: 'kubernetes.io/hostname'  # This generates default anti-affinity rules
  #     zones:  # Zone list has to be fully redefined for modification. Update with you actual zones or skip to use logical zones only.
  #     - name: zone-a
  #       nodeSelector:
  #         topology.kubernetes.io/zone: us-central1-a
  #     - name: zone-a
  #       nodeSelector:
  #         topology.kubernetes.io/zone: us-central1-b
  #     - name: zone-c
  #       nodeSelector:
  #         topology.kubernetes.io/zone: us-central1-c
  #
  zoneAwareReplication:
    # -- Enable zone-aware replication for alertmanager
    enabled: false
    # -- Maximum number of alertmanagers that can be unavailable per zone during rollout
    maxUnavailable: 2
    # -- topologyKey to use in pod anti-affinity. If unset, no anti-affinity rules are generated. If set, the generated anti-affinity rule makes sure that pods from different zones do not mix.
    # E.g.: topologyKey: 'kubernetes.io/hostname'
    topologyKey: null
    # -- Auxiliary values for migration, see https://grafana.com/docs/helm-charts/mimir-distributed/latest/migration-guides/migrate-from-single-zone-with-helm/
    migration:
      # -- Indicate if migration is ongoing for multi zone alertmanager
      enabled: false
      # -- Start zone-aware alertmanagers
      writePath: false
    # -- Zone definitions for alertmanager zones. Note: you have to redefine the whole list to change parts as YAML does not allow to modify parts of a list.
    zones:
    # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    - name: zone-a
      # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
      # nodeSelector:
      #   topology.kubernetes.io/zone: zone-a
      nodeSelector: null
      # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
      extraAffinity: {}
      # -- Alertmanager data Persistent Volume Storage Class
      # If defined, storageClassName: <storageClass>
      # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
      # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.
      storageClass: null
    # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    - name: zone-b
      # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
      # nodeSelector:
      #   topology.kubernetes.io/zone: zone-b
      nodeSelector: null
      # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
      extraAffinity: {}
      # -- Alertmanager data Persistent Volume Storage Class
      # If defined, storageClassName: <storageClass>
      # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
      # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.
      storageClass: null
    # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    - name: zone-c
      # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
      # nodeSelector:
      #   topology.kubernetes.io/zone: zone-c
      nodeSelector: null
      # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
      extraAffinity: {}
      # -- Alertmanager data Persistent Volume Storage Class
      # If defined, storageClassName: <storageClass>
      # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
      # If undefined or set to null (the default), then fall back to the value of `alertmanager.persistentVolume.storageClass`.
      storageClass: null

distributor:
  replicas: 1

  service:
    annotations: {}
    labels: {}

  resources:
    requests:
      cpu: 100m
      memory: 512Mi

  # Additional distributor container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    log.level: debug

  persistence:
    subPath:

ingester:
  replicas: 1
  statefulSet:
    enabled: true

  resources:
    requests:
      cpu: 100m
      memory: 512Mi

  persistentVolume:
    enabled: true
    size: 2Gi
    storageClass: "standard"

  zoneAwareReplication:
    enabled: false

overrides_exporter:
  enabled: true
  replicas: 1

  persistence:
    subPath:

  resources:
    requests:
      cpu: 100m
      memory: 128Mi

ruler:
  enabled: true
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 128Mi

  persistence:
    subPath:

querier:
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 128Mi

  persistence:
    subPath:

query_frontend:
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 128Mi

  persistence:
    subPath:

query_scheduler:
  enabled: true
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 128Mi

  persistence:
    subPath:

store_gateway:
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 512Mi

  persistentVolume:
    enabled: true
    size: 2Gi
    storageClass: "standard"

  zoneAwareReplication:
    enabled: false

compactor:
  replicas: 1

  resources:
    requests:
      cpu: 100m
      memory: 512Mi

  persistentVolume:
    enabled: true
    size: 2Gi
    storageClass: "standard"

memcached:
  image:
    repository: memcached
    tag: 1.6.19-alpine
    pullPolicy: IfNotPresent

memcachedExporter:
  enabled: true

  image:
    repository: prom/memcached-exporter
    tag: v0.11.2
    pullPolicy: IfNotPresent

  resources:
    requests: {}
    limits: {}

chunks-cache:
  enabled: false
  replicas: 1
  # -- Amount of memory allocated to chunks-cache for object storage (in MB).
  allocatedMemory: 8192
  # -- Maximum item memory for chunks-cache (in MB).
  maxItemMemory: 1
  resources: null

index-cache:
  enabled: false
  replicas: 1
  # -- Amount of memory allocated to index-cache for object storage (in MB).
  allocatedMemory: 2048
  # -- Maximum item index-cache for memcached (in MB).
  maxItemMemory: 5
  resources: null

metadata-cache:
  enabled: false
  replicas: 1
  # -- Amount of memory allocated to metadata-cache for object storage (in MB).
  allocatedMemory: 512
  # -- Maximum item metadata-cache for memcached (in MB).
  maxItemMemory: 1
  resources: null

results-cache:
  enabled: true
  replicas: 1
  # -- Amount of memory allocated to results-cache for object storage (in MB).
  allocatedMemory: 512
  # -- Maximum item results-cache for memcached (in MB).
  maxItemMemory: 5
  resources: null

rollout_operator:
  enabled: true

minio:
  enabled: false

nginx:
  enabled: false

gateway:
  enabledNonEnterprise: true
  replicas: 1

  autoscaling:
    enabled: false
  resources: {}

  ingress:
    enabled: false
    ingressClassName: 'contour'
    hosts:
      - host: mimir.loc
        paths:
          - path: /
            pathType: Prefix
    tls:
      - secretName: mimir-tls
        hosts:
          - mimir.loc

metaMonitoring:
  dashboards:
    enabled: false
    # -- Annotations to add to the Grafana dashboard ConfigMap
    annotations:
      k8s-sidecar-target-directory: /tmp/dashboards/Mimir Dashboards
    # -- Labels to add to the Grafana dashboard ConfigMap
    labels:
      grafana_dashboard: "1"

  # ServiceMonitor configuration for monitoring Kubernetes Services with Prometheus Operator and/or Grafana Agent
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: false
    # -- To disable setting a 'cluster' label in metrics, set to 'null'.
    # To overwrite the 'cluster' label with your own value, set to a non-empty string.
    # Keep empty string "" to have the default value in the 'cluster' label, which is the helm release name for Mimir and the actual cluster name for Enterprise Metrics.
    clusterLabel: ""
    # -- Alternative namespace for ServiceMonitor resources
    # If left unset, the default is to install the ServiceMonitor resources in the namespace where the chart is installed, i.e. the namespace specified for the helm command.
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    # If left unset, the default is to select the namespace where the chart is installed, i.e. the namespace specified for the helm command.
    namespaceSelector: null
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to targets before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig
    relabelings: []
    # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.RelabelConfig
    metricRelabelings: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig: null

  # Rules for the Prometheus Operator
  prometheusRule:
    # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
    enabled: false
    # -- Create standard Mimir alerts in Prometheus Operator via a PrometheusRule CRD
    mimirAlerts: false
    # -- Create standard Mimir recording rules in Prometheus Operator via a PrometheusRule CRD
    mimirRules: false
    # -- PrometheusRule annotations
    annotations: {}
    # -- Additional PrometheusRule labels. To find out what your Prometheus operator expects,
    # see the Prometheus object and field spec.ruleSelector
    labels: {}
    # -- prometheusRule namespace. This should be the namespace where the Prometheus Operator is installed,
    # unless the Prometheus Operator is set up to look for rules outside its namespace
    namespace: null
    # -- Contents of Prometheus rules file
    groups: []

  # metaMonitoringAgent configures the built in Grafana Agent that can scrape metrics and logs and send them to a local or remote destination
  grafanaAgent:
    # -- Controls whether to create PodLogs, MetricsInstance, LogsInstance, and GrafanaAgent CRs to scrape the
    # ServiceMonitors of the chart and ship metrics and logs to the remote endpoints below.
    # Note that you need to configure serviceMonitor in order to have some metrics available.
    enabled: false

    # -- Controls the image repository and tag for config-reloader and grafana-agent containers in the meta-monitoring
    # StatefulSet and DaemonSet created by the grafana-agent-operator. You can define one or both sections under imageRepo.
    # If a section is defined, you must pass repo, image and tag keys.
    imageRepo:
    #  configReloader:
    #    repo: quay.io
    #    image: prometheus-operator/prometheus-config-reloader
    #    tag: v0.47.0
    #  grafanaAgent:
    #    repo: docker.io
    #    image: grafana/agent
    #    tag: v0.29.0

    # -- Controls whether to install the Grafana Agent Operator and its CRDs.
    # Note that helm will not install CRDs if this flag is enabled during an upgrade.
    # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds
    installOperator: false

    logs:
      # -- Controls whether to create resources PodLogs and LogsInstance resources
      enabled: true

      # -- Default destination for logs. The config here is translated to Promtail client
      # configuration to write logs to this Loki-compatible remote. Optional.
      remote:
        # -- Full URL for Loki push endpoint. Usually ends in /loki/api/v1/push
        url: ''

        auth:
          # -- Used to set X-Scope-OrgID header on requests. Usually not used in combination with username and password.
          tenantId: ''

          # -- Basic authentication username. Optional.
          username: ''

          # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.
          passwordSecretName: ''
          # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.
          passwordSecretKey: ''

      # -- Client configurations for the LogsInstance that will scrape Mimir pods. Follows the format of .remote.
      additionalClientConfigs: []

    metrics:
      # -- Controls whether to create MetricsInstance resources and ServiceMonitor resources for scraping Kubernetes (when .scrapeK8s.enabled=true).
      enabled: true

      # -- Default destination for metrics. The config here is translated to remote_write
      # configuration to push metrics to this Prometheus-compatible remote. Optional.
      # Note that you need to configure serviceMonitor in order to have some metrics available.
      #
      # If you leave the metamonitoring.grafanaAgent.metrics.remote.url field empty,
      # then the chart automatically fills in the address of the GEM gateway Service
      # or the Mimir NGINX Service.
      #
      # If you have deployed Mimir, and metamonitoring.grafanaAgent.metrics.remote.url is not set,
      # then the metamonitoring metrics are be sent to the Mimir cluster.
      # You can query these metrics using the HTTP header X-Scope-OrgID: metamonitoring
      #
      # If you have deployed GEM, then there are two cases:
      # * If are using the 'trust' authentication type (mimir.structuredConfig.auth.type: trust),
      #   then the same instructions apply as for Mimir.
      #
      # * If you are using the enterprise authentication type (mimir.structuredConfig.auth.type=enterprise, which is also the default when enterprise.enabled=true),
      #   then you also need to provide a Secret with the authentication token for the tenant.
      #   The token should be to an access policy with metrics:read scope.
      #   To set up the Secret, refer to https://grafana.com/docs/helm-charts/mimir-distributed/latest/run-production-environment-with-helm/monitor-system-health/
      #   Assuming you are using the GEM authentication model, the Helm chart values should look like the following example.
      #
      # remote:
      #   auth:
      #     username: metamonitoring
      #     passwordSecretName: gem-tokens
      #     passwordSecretKey: metamonitoring
      remote:
        # -- Full URL for Prometheus remote-write. Usually ends in /push.
        # If you leave the url field empty, then the chart automatically fills in the
        # address of the GEM gateway Service or the Mimir NGINX Service.
        url: ''

        # -- Used to add HTTP headers to remote-write requests.
        headers: {}
        auth:
          # -- Basic authentication username. Optional.
          username: ''

          # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.
          passwordSecretName: ''
          # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.
          passwordSecretKey: ''

      # -- Additional remote-write for the MetricsInstance that will scrape Mimir pods. Follows the format of .remote.
      additionalRemoteWriteConfigs: []

      scrapeK8s:
        # -- When grafanaAgent.enabled and serviceMonitor.enabled, controls whether to create ServiceMonitors CRs
        # for cadvisor, kubelet, and kube-state-metrics. The scraped metrics are reduced to those pertaining to
        # Mimir pods only.
        enabled: true

        # -- Controls service discovery of kube-state-metrics.
        kubeStateMetrics:
          namespace: kube-system
          labelSelectors:
            app.kubernetes.io/name: kube-state-metrics

      # -- The scrape interval for all ServiceMonitors.
      scrapeInterval: 60s

    # -- Sets the namespace of the resources. Leave empty or unset to use the same namespace as the Helm release.
    namespace: ''

    # -- Labels to add to all monitoring.grafana.com custom resources.
    # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.labels for that.
    labels: {}

    # -- Annotations to add to all monitoring.grafana.com custom resources.
    # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.annotations for that.
    annotations: {}

    # -- SecurityContext of Grafana Agent pods. This is different from the SecurityContext that the operator pod runs with.
    # The operator pod SecurityContext is configured in the grafana-agent-operator.podSecurityContext value.
    # As of mimir-distributed 4.0.0 the Agent DaemonSet that collects logs needs to run as root and be able to access the
    # pod logs on each host. Because of that the agent subchart is incompatible with the PodSecurityPolicy of the
    # mimir-distributed chart and with the Restricted policy of Pod Security Standards https://kubernetes.io/docs/concepts/security/pod-security-standards/
    podSecurityContext:
    #  fsGroup: 10001
    #  runAsGroup: 10001
    #  runAsNonRoot: true
    #  runAsUser: 10001
    #  seccompProfile:
    #    type: RuntimeDefault

    # -- SecurityContext of Grafana Agent containers. This is different from the SecurityContext that the operator container runs with.
    # As of mimir-distributed 4.0.0 the agent subchart needs to have root file system write access so that the Agent pods can write temporary files where.
    # This makes the subchart incompatible with the PodSecurityPolicy of the mimir-distributed chart.
    containerSecurityContext:
    #  allowPrivilegeEscalation: false
    #  runAsUser: 10001
    #  capabilities:
    #    drop: [ALL]

# Settings for the initial admin(istrator) token generator job. Can only be enabled if
# enterprise.enabled is true - requires license.
tokengenJob:
  enable: true
  extraArgs: {}
  env: []
  extraEnvFrom: []
  annotations: {}
  initContainers: []

  # -- SecurityContext override for tokengenjob pods
  securityContext: {}

  # -- The SecurityContext for tokengenjob containers
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]

  # -- The name of the PriorityClass for tokenjobgen pods
  priorityClassName: null

# -- Cache for admin bucket.
# If this is disabled, in-memory cache will be set by default.
# You can use Redis too for cache and set the configuration via structuredConfig.
# See GEM documentation for Redis configuration option.
admin-cache:
  # -- Specifies whether admin-cache using memcached should be enabled
  enabled: false

  # -- Total number of admin-cache replicas
  replicas: 1

  # -- Port of the admin-cache service
  port: 11211

  # -- Amount of memory allocated to admin-cache for object storage (in MB).
  allocatedMemory: 64

  # -- Maximum item memory for admin-cache (in MB).
  maxItemMemory: 1

  # -- Extra init containers for admin-cache pods
  initContainers: []

  # -- Annotations for the admin-cache pods
  annotations: {}
  # -- Node selector for admin-cache pods
  nodeSelector: {}
  # -- Affinity for admin-cache pods
  affinity: {}

  # -- topologySpreadConstraints allows to customize the default topologySpreadConstraints. This can be either a single dict as shown below or a slice of topologySpreadConstraints.
  # labelSelector is taken from the constraint itself (if it exists) or is generated by the chart using the same selectors as for services.
  topologySpreadConstraints: {}
  #  maxSkew: 1
  #  topologyKey: kubernetes.io/hostname
  #  whenUnsatisfiable: ScheduleAnyway

  # -- Tolerations for admin-cache pods
  tolerations: []
  # -- Pod Disruption Budget
  podDisruptionBudget:
    maxUnavailable: 1
  # -- The name of the PriorityClass for admin-cache pods
  priorityClassName: null
  # -- Labels for admin-cache pods
  podLabels: {}
  # -- Annotations for admin-cache pods
  podAnnotations: {}
  # -- Management policy for admin-cache pods
  podManagementPolicy: Parallel
  # -- Grace period to allow the admin-cache to shutdown before it is killed
  terminationGracePeriodSeconds: 60

  # -- Stateful admin-cache strategy
  statefulStrategy:
    type: RollingUpdate

  # -- Additional CLI args for admin-cache
  extraArgs: {}

  # -- Additional containers to be added to the admin-cache pod.
  extraContainers: []

  # -- Resource requests and limits for the admin-cache
  # By default a safe memory limit will be requested based on allocatedMemory value (floor (* 1.2 allocatedMemory)).
  resources: null

  # -- Service annotations and labels
  service:
    annotations: {}
    labels: {}

graphite:
  enabled: false

gr-aggr-cache:
  enabled: false

gr-metricname-cache:
  enabled: false

# -- Settings for the smoke-test job. This is meant to run as a Helm test hook
# (`helm test RELEASE`) after installing the chart. It quickly verifies
# that writing and reading metrics works. Currently not supported for
# installations using GEM token-based authentication.
smoke_test:
  image:
    repository: grafana/mimir-continuous-test
    tag: 2.10.4
    pullPolicy: IfNotPresent
  tenantId: ''
  extraArgs: {}
  env: []
  extraEnvFrom: []
  annotations: {}
  initContainers: []
  # -- The name of the PriorityClass for smoke-test pods
  priorityClassName: null

continuous_test:
  enabled: false
